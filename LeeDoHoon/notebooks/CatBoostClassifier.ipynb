{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# imports + seed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        ")\n",
        "\n",
        "RANDOM_STATE = 719\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option('display.max_rows', None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_parquet('kkbox_train_feature_v1.parquet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) bd vs bd_clean 중복 제거\n",
        "if \"bd\" in df.columns and \"bd_clean\" in df.columns:\n",
        "    df = df.drop(columns=[\"bd\"])\n",
        "\n",
        "# 2) time / month 숫자화\n",
        "df[\"reg_year\"]  = df[\"registration_init_time\"].dt.year.astype(\"Int64\")\n",
        "\n",
        "# 원본 제거 (Period/Datetime 에러 방지)\n",
        "df = df.drop(columns=[\"registration_init_time\", \"registration_month\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# train/valid/test split (stratify 유지)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert \"msno\" in df.columns and \"is_churn\" in df.columns\n",
        "\n",
        "trainval_df, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.15,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=df[\"is_churn\"],\n",
        ")\n",
        "\n",
        "valid_size = 0.15 / 0.85\n",
        "train_df, valid_df = train_test_split(\n",
        "    trainval_df,\n",
        "    test_size=valid_size,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=trainval_df[\"is_churn\"],\n",
        ")\n",
        "\n",
        "feature_cols = [c for c in df.columns if c not in [\"msno\", \"is_churn\"]]\n",
        "\n",
        "X_train, y_train = train_df[feature_cols], train_df[\"is_churn\"].astype(int)\n",
        "X_valid, y_valid = valid_df[feature_cols], valid_df[\"is_churn\"].astype(int)\n",
        "X_test,  y_test  = test_df[feature_cols],  test_df[\"is_churn\"].astype(int)\n",
        "\n",
        "print(\"churn rate:\", y_train.mean(), y_valid.mean(), y_test.mean())\n",
        "print(X_train.shape, X_valid.shape, X_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# column split + preprocess (OHE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pandas.api.types import is_numeric_dtype\n",
        "\n",
        "cat_cols = [c for c in X_train.columns if not is_numeric_dtype(X_train[c])]\n",
        "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
        "\n",
        "print(\"num:\", len(num_cols), \"cat:\", len(cat_cols))\n",
        "print(\"cat example:\", cat_cols[:10])\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", Pipeline([\n",
        "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        ]), num_cols),\n",
        "        (\"cat\", Pipeline([\n",
        "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)),\n",
        "        ]), cat_cols),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 공통 평가 함수\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_binary(y_true, p_pred, prefix=\"\", thr=0.5):\n",
        "    roc = roc_auc_score(y_true, p_pred)\n",
        "    ap  = average_precision_score(y_true, p_pred)\n",
        "\n",
        "    y_hat = (p_pred >= thr).astype(int)\n",
        "    cm = confusion_matrix(y_true, y_hat)\n",
        "    cr = classification_report(y_true, y_hat, digits=4)\n",
        "\n",
        "    print(f\"{prefix}ROC-AUC: {roc:.6f} | PR-AUC(AP): {ap:.6f} | thr={thr}\")\n",
        "    print(f\"{prefix}Confusion matrix:\\n{cm}\")\n",
        "    print(f\"{prefix}Classification report:\\n{cr}\")\n",
        "    return roc, ap, cm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CatBoost 학습\n",
        "\n",
        "- loss_function: \"Logloss\"\n",
        "- eval_metric: \"AUC\"\n",
        "- scale_pos_weight로 불균형 대응 (Recall 최적화)\n",
        "- early_stopping_rounds=50 적용\n",
        "- thread_count=-1로 풀코어\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# scale_pos_weight 계산 (Recall 최적화)\n",
        "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "print(f\"scale_pos_weight: {scale_pos_weight:.2f}\")\n",
        "\n",
        "# 전처리 적용\n",
        "X_train_prep = preprocess.fit_transform(X_train)\n",
        "X_valid_prep = preprocess.transform(X_valid)\n",
        "X_test_prep = preprocess.transform(X_test)\n",
        "\n",
        "print(f\"Preprocessed shapes: {X_train_prep.shape}, {X_valid_prep.shape}, {X_test_prep.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cb_model = CatBoostClassifier(\n",
        "    loss_function=\"Logloss\",\n",
        "    eval_metric=\"AUC\",\n",
        "    learning_rate=0.05,\n",
        "    depth=6,\n",
        "    l2_leaf_reg=3.0,\n",
        "    iterations=500,\n",
        "    early_stopping_rounds=50,\n",
        "    scale_pos_weight=float(scale_pos_weight),\n",
        "    random_seed=RANDOM_STATE,\n",
        "    thread_count=-1,\n",
        "    verbose=100,\n",
        ")\n",
        "\n",
        "cb_model.fit(\n",
        "    X_train_prep,\n",
        "    y_train,\n",
        "    eval_set=(X_valid_prep, y_valid),\n",
        "    use_best_model=True,\n",
        ")\n",
        "\n",
        "print(f\"\\nBest iteration: {cb_model.best_iteration_}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 예측 및 평가\n",
        "p_valid = cb_model.predict_proba(X_valid_prep)[:, 1]\n",
        "p_test  = cb_model.predict_proba(X_test_prep)[:, 1]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Threshold = 0.5\")\n",
        "print(\"=\" * 60)\n",
        "eval_binary(y_valid, p_valid, prefix=\"[CB valid] \")\n",
        "eval_binary(y_test,  p_test,  prefix=\"[CB test ] \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Threshold 최적화 (Recall 우선)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve, recall_score, precision_score, f1_score\n",
        "\n",
        "def find_optimal_threshold_for_recall(y_true, y_prob, target_recall=0.95):\n",
        "    \"\"\"목표 Recall을 달성하는 최적 Threshold 찾기\"\"\"\n",
        "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_prob)\n",
        "    \n",
        "    # target_recall 이상인 threshold 중 가장 높은 precision\n",
        "    valid_idx = np.where(recalls >= target_recall)[0]\n",
        "    if len(valid_idx) == 0:\n",
        "        best_idx = np.argmax(recalls)\n",
        "    else:\n",
        "        best_idx = valid_idx[np.argmax(precisions[valid_idx])]\n",
        "    \n",
        "    if best_idx >= len(thresholds):\n",
        "        best_idx = len(thresholds) - 1\n",
        "    \n",
        "    return thresholds[best_idx]\n",
        "\n",
        "# 목표: Recall 95%\n",
        "optimal_thr = find_optimal_threshold_for_recall(y_valid, p_valid, target_recall=0.95)\n",
        "print(f\"Optimal threshold for Recall >= 95%: {optimal_thr:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"Threshold = {optimal_thr:.2f} (Recall 최적화)\")\n",
        "print(\"=\" * 60)\n",
        "eval_binary(y_valid, p_valid, prefix=\"[CB valid] \", thr=optimal_thr)\n",
        "eval_binary(y_test,  p_test,  prefix=\"[CB test ] \", thr=optimal_thr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature importance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# preprocess의 출력 피처명 얻기\n",
        "feature_names = preprocess.get_feature_names_out()\n",
        "\n",
        "importances = cb_model.get_feature_importance()\n",
        "imp_df = pd.DataFrame({\n",
        "    \"feature\": feature_names,\n",
        "    \"importance\": importances,\n",
        "}).sort_values(\"importance\", ascending=False)\n",
        "\n",
        "imp_df.head(30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "imp_df.to_csv(\n",
        "    \"../data/model_df/cb_feature_importance_split_preprocessed.csv\",\n",
        "    index=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Permutation importance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JOBLIB_TEMP_FOLDER\"] = r\"C:\\joblib_tmp\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# 너무 느리면 valid에서 일부만 샘플링\n",
        "sample_n = 30000\n",
        "if X_valid_prep.shape[0] > sample_n:\n",
        "    idx = np.random.choice(X_valid_prep.shape[0], size=sample_n, replace=False)\n",
        "    Xv = X_valid_prep[idx]\n",
        "    yv = y_valid.iloc[idx]\n",
        "else:\n",
        "    Xv, yv = X_valid_prep, y_valid\n",
        "\n",
        "perm = permutation_importance(\n",
        "    cb_model,\n",
        "    Xv, yv,\n",
        "    n_repeats=5,\n",
        "    random_state=RANDOM_STATE,\n",
        "    scoring=\"roc_auc\",\n",
        "    n_jobs=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "perm_df = pd.DataFrame({\n",
        "    \"feature\": feature_names,\n",
        "    \"perm_importance_mean\": perm.importances_mean,\n",
        "    \"perm_importance_std\": perm.importances_std,\n",
        "}).sort_values(\"perm_importance_mean\", ascending=False)\n",
        "\n",
        "perm_df.head(30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "perm_df.to_csv(\"../data/model_df/cb_perm_importance.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 모델 저장\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import joblib\n",
        "\n",
        "# 모델 저장 디렉토리\n",
        "out_dir = Path(r\"C:\\artifacts\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# CatBoost 모델 저장 (.cbm 형식)\n",
        "cb_model_path = out_dir / \"catboost_model.cbm\"\n",
        "cb_model.save_model(str(cb_model_path))\n",
        "print(f\"CatBoost model saved: {cb_model_path}\")\n",
        "\n",
        "# 전처리 파이프라인 저장\n",
        "prep_path = out_dir / \"catboost_preprocessor.joblib\"\n",
        "joblib.dump(preprocess, prep_path, compress=3)\n",
        "print(f\"Preprocessor saved: {prep_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 로드 예시\n",
        "# from catboost import CatBoostClassifier\n",
        "# import joblib\n",
        "# \n",
        "# cb_model = CatBoostClassifier()\n",
        "# cb_model.load_model(r\"C:\\artifacts\\catboost_model.cbm\")\n",
        "# preprocess = joblib.load(r\"C:\\artifacts\\catboost_preprocessor.joblib\")\n",
        "# \n",
        "# X_new_prep = preprocess.transform(X_new)\n",
        "# p = cb_model.predict_proba(X_new_prep)[:, 1]\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
